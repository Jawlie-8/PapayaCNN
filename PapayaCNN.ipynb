{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b139aadc-9ec1-40a8-8c96-e0756017670a",
   "metadata": {},
   "source": [
    "# PapayaCNN \n",
    "> Development of a Papaya Image Classifier using CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36067865-5600-40dd-b486-254c826f7867",
   "metadata": {},
   "source": [
    "## Revision History\n",
    "\n",
    "- yyyy/mm/dd Activity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb55945-9fb3-43fe-9fa4-b0b2756bc8bc",
   "metadata": {},
   "source": [
    "## Step 1. Install Dependencies \n",
    "\n",
    "> Install the required third-party libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86186908-77f3-4c30-a69a-31add5842dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!update.cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa285b-b174-4fcb-9e02-799c1901eac0",
   "metadata": {},
   "source": [
    "## Step 2: Import the Packages \n",
    " \n",
    "> Import all third-party libraries necessary for the CNN model to execute successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e9915-59a7-460a-8ebf-831c3c9491b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    " \n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\" \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n",
    " \n",
    "import glob \n",
    "import json \n",
    "import time \n",
    "import warnings \n",
    "from datetime import datetime \n",
    " \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"os.fork()\") \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Your `PyDataset` class should call\") \n",
    " \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.regularizers import l2 \n",
    "from tensorflow.keras.models import Sequential \n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras.layers import ( \n",
    "    Input, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense \n",
    ") \n",
    "from tensorflow.keras.optimizers import Adadelta \n",
    " \n",
    " \n",
    "gpus = tf.config.list_physical_devices(\"GPU\") \n",
    " \n",
    "if gpus: \n",
    "    try: \n",
    "        for gpu in gpus: \n",
    "            tf.config.experimental.set_memory_growth(gpu, True) \n",
    "    except RuntimeError as e: \n",
    "            print(e) \n",
    "    print(\"GPU detected. Running on GPU.\") \n",
    "else: \n",
    "    print(\"No GPU detected. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cfff4-8bde-4152-989f-3fd18d406159",
   "metadata": {},
   "source": [
    "## Step 3: Load Datasets \n",
    " \n",
    "> Load and prepare the training, validation, and testing datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c6482-7f5b-4131-8c8a-6efc192cb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\" \n",
    "IMAGE_SIZE = 224 \n",
    "IMAGE_CHANNELS = 3 \n",
    "BATCH_SIZE = 8 \n",
    " \n",
    "all_images = glob.glob(f\"{DATA_DIR}/raw/*/*.jpg\") \n",
    "df = pd.DataFrame({ \n",
    "    \"filepath\": all_images, \n",
    "    \"label\": [os.path.basename(os.path.dirname(p)) for p in all_images] \n",
    "}) \n",
    " \n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, stratify=df[\"label\"], shuffle=True) \n",
    "train_df, val_df = train_test_split( \n",
    "                       train_val_df, \n",
    "                       test_size=0.2, \n",
    "                       stratify=train_val_df[\"label\"], \n",
    "                       shuffle=True) \n",
    " \n",
    "generator1 = ImageDataGenerator( \n",
    "    rescale=1./255, \n",
    "    rotation_range=20, \n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2, \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.2, \n",
    "    horizontal_flip=True, \n",
    "    fill_mode=\"nearest\" \n",
    ") \n",
    " \n",
    "train_data = generator1.flow_from_dataframe( \n",
    "    train_df, x_col=\"filepath\", y_col=\"label\", target_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "    batch_size=BATCH_SIZE, class_mode=\"categorical\", shuffle=True \n",
    ") \n",
    " \n",
    "NUM_CLASSES = max(train_data.classes) + 1 \n",
    " \n",
    "generator2 = ImageDataGenerator(rescale=1./255) \n",
    " \n",
    "val_data = generator2.flow_from_dataframe( \n",
    "    val_df, x_col=\"filepath\", y_col=\"label\", target_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "    batch_size=BATCH_SIZE, class_mode=\"categorical\", shuffle=True \n",
    ") \n",
    " \n",
    "test_data = generator2.flow_from_dataframe( \n",
    "    test_df, x_col=\"filepath\", y_col=\"label\", target_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "    batch_size=BATCH_SIZE, class_mode=\"categorical\", shuffle=False \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29ebf3-8b2f-43a6-b33e-33f598cb7e7d",
   "metadata": {},
   "source": [
    "## Step 4: Define the Architecture \n",
    " \n",
    "> Define the structure of the CNN for Papaya classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58af39-0120-4e5b-8464-f15eec3a03db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    " \n",
    "model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))) \n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation=\"swish\", padding=\"same\")) \n",
    "model.add(Conv2D(32, (3, 3), activation=\"swish\", padding=\"same\")) \n",
    "model.add(Conv2D(32, (3, 3), activation=\"swish\", padding=\"same\")) \n",
    "model.add(MaxPooling2D((2, 2))) \n",
    " \n",
    "model.add(Conv2D(64, (3, 3), activation=\"swish\", padding=\"same\")) \n",
    "model.add(MaxPooling2D((2, 2))) \n",
    "model.add(Dropout(0.1)) \n",
    " \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(32, activation=\"swish\")) \n",
    "model.add(Dense(32, activation=\"swish\")) \n",
    "model.add(Dropout(0.1)) \n",
    " \n",
    "model.add(Dense(NUM_CLASSES, activation=\"softmax\")) \n",
    " \n",
    "optimizer = Adadelta(learning_rate=1.0, rho=0.95) \n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", \n",
    "metrics=[\"accuracy\"]) \n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba86ae6-c813-4c4f-b81b-3a3927c0cd60",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model   \n",
    " \n",
    "> Feed the training-val dataset to the compiled CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b64ea2-b4d6-4689-9137-431fb8074231",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100 \n",
    "MODELS = \"models\" \n",
    "ANALYSIS = \"analysis\" \n",
    "ARCHITECTURE = \"CNN\" \n",
    " \n",
    "os.makedirs(MODELS, exist_ok=True) \n",
    "os.makedirs(ANALYSIS, exist_ok=True) \n",
    " \n",
    "training_timestamp = int(time.time()) \n",
    " \n",
    "history = model.fit(train_data, validation_data=val_data, epochs=EPOCHS) \n",
    " \n",
    "training_duration = (int(time.time()) - training_timestamp) / 60\n",
    "\n",
    "fullpath = f\"{MODELS}/topic.{ARCHITECTURE}_{training_timestamp}.keras\" \n",
    "model.save(fullpath) \n",
    " \n",
    "with open(f\"{ANALYSIS}/metrics_{training_timestamp}.json\", \"w\") as f: \n",
    "    json.dump({ \n",
    "        \"loss\": history.history[\"loss\"], \n",
    "        \"accuracy\": history.history[\"accuracy\"], \n",
    "        \"val_loss\": history.history[\"val_loss\"], \n",
    "        \"val_accuracy\": history.history[\"val_accuracy\"] \n",
    "    }, f, indent=4) \n",
    " \n",
    "print(f\"Training completed in {training_duration:.2f} minutes.\") \n",
    "print(f\"Metrics saved to '{ANALYSIS}/metrics_{training_timestamp}.json'\") \n",
    "print(f\"Model saved to '{fullpath}'\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742275e-0468-439b-bea4-f27c708d3243",
   "metadata": {},
   "source": [
    "## Step 6: Generate Training Analysis   \n",
    " \n",
    "**Metrics Definitions**   \n",
    " - Loss is computed based on how far each prediction is from the ground truth, \n",
    "specifically using Categorical Cross-entropy. - Accuracy is the proportion of correct predictions to the total predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4cd6f-fa33-474b-9d42-c778dde34769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    " \n",
    "with open(f\"{ANALYSIS}/metrics_{training_timestamp}.json\", \"r\") as f: \n",
    "    metrics = json.load(f) \n",
    " \n",
    "epochs = [i for i in range(1, len(metrics[\"loss\"])+1)] \n",
    " \n",
    "plt.figure(figsize=(8, 6)) \n",
    "sns.lineplot(x=epochs, y=metrics[\"loss\"], label=\"Training Loss\", color=\"blue\") \n",
    "sns.lineplot(x=epochs, y=metrics[\"val_loss\"], label=\"Validation Loss\", color=\"orange\") \n",
    "plt.title(\"Loss\") \n",
    "plt.xlabel(\"Epochs\") \n",
    "plt.ylabel(\"Loss\") \n",
    "plt.legend() \n",
    "plt.grid(True) \n",
    "plt.savefig(f\"{ANALYSIS}/loss_plot_{training_timestamp}.png\") \n",
    "plt.show() \n",
    "\n",
    "plt.figure(figsize=(8, 6)) \n",
    "sns.lineplot(x=epochs, y=metrics[\"accuracy\"], label=\"Training Accuracy\", color=\"green\") \n",
    "sns.lineplot(x=epochs, y=metrics[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"red\") \n",
    "plt.title(\"Accuracy\") \n",
    "plt.xlabel(\"Epochs\") \n",
    "plt.ylabel(\"Accuracy\") \n",
    "plt.legend() \n",
    "plt.grid(True) \n",
    "plt.savefig(f\"{ANALYSIS}/accuracy_plot_{training_timestamp}.png\") \n",
    "plt.show() \n",
    " \n",
    "print(f\"\\nPlots saved to {ANALYSIS} directory.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad608db-2d8d-4f06-bae8-d3743cbe3ac6",
   "metadata": {},
   "source": [
    "## Step 7: Test the Model \n",
    " \n",
    "> Run the model using the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8c6e7-2e28-4ad9-a301-a1547538e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    " \n",
    "results = model.predict(test_data, verbose=1) \n",
    " \n",
    "prediction_duration = time.time() - start_time \n",
    "image_prediction_time = prediction_duration / test_data.samples \n",
    " \n",
    "predictions = (results > 0.5).astype(\"int32\").flatten() if results.shape[1] == 1 else results.argmax(axis=1) \n",
    " \n",
    "print(f\"Total prediction time: {prediction_duration:.4f} seconds\") \n",
    "print(f\"Time per image: {image_prediction_time:.4f} seconds\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c2c8c-5053-4460-a91e-f7423524b4ee",
   "metadata": {},
   "source": [
    "## Step 8: Display the Results \n",
    " \n",
    "> Show the actual classes and predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9792d718-ef6b-4dc7-88d7-05f98286f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg \n",
    "from PIL import Image \n",
    " \n",
    " \n",
    "for fp, true, pred in zip(filepaths, true_indices, predictions): \n",
    "    img = Image.open(fp).resize((224, 224)) \n",
    " \n",
    "    filename = os.path.basename(fp) \n",
    "    actual = labels[true].replace(\"-\", \" \").title() \n",
    "    predicted = labels[pred].replace(\"-\", \" \").title() \n",
    "     \n",
    "    plt.figure(figsize=(2.5, 3)) \n",
    "    plt.imshow(img) \n",
    "    plt.axis('off') \n",
    " \n",
    "    text = f\"{filename}\\nActual: {actual}\\nPredicted: {predicted}\" \n",
    "    plt.text(0.5, -0.1, text, fontsize=8, ha=\"center\", va=\"top\", transform=plt.gca().transAxes) \n",
    " \n",
    "    plt.tight_layout() \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7cbf1-d8ca-4a3a-bafc-186f1aa4e326",
   "metadata": {},
   "source": [
    "## Step 9: Confusion Matrix with Cohen's Kappa Score Analysis \n",
    " \n",
    "> The ideal matrix is a left-to-right diagonal; however, the Cohen's Kappa score is \n",
    "calculated to quantify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95095e43-a822-4962-9bdb-86e5beb1cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ( \n",
    "    confusion_matrix, ConfusionMatrixDisplay, cohen_kappa_score \n",
    ") \n",
    " \n",
    " \n",
    "true_indices = test_data.classes \n",
    "class_labels = list(test_data.class_indices.keys()) \n",
    " \n",
    "cm = confusion_matrix(true_indices, predictions) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels) \n",
    "kappa_score = cohen_kappa_score(true_indices, predictions) \n",
    " \n",
    "fig, ax = plt.subplots(figsize=(8, 8)) \n",
    "\n",
    "disp.plot(cmap=\"magma\", ax=ax) \n",
    " \n",
    "plt.title(f\"Confusion Matrix with Cohen's Kappa: {kappa_score:.2f}\") \n",
    "plt.savefig(f\"{ANALYSIS}/confusion-matrix_{training_timestamp}.png\") \n",
    "plt.show() \n",
    " \n",
    "# Landis & Koch (1977) scale \n",
    "kappa_scale = { \n",
    "    (-1.0, 0.00): \"Poor agreement\", \n",
    "    (0.00, 0.20): \"Slight agreement\", \n",
    "    (0.21, 0.40): \"Fair agreement\", \n",
    "    (0.41, 0.60): \"Moderate agreement\", \n",
    "    (0.61, 0.80): \"Substantial agreement\", \n",
    "    (0.81, 1.00): \"Almost perfect agreement\" \n",
    "} \n",
    " \n",
    "for interval, label in kappa_scale.items(): \n",
    "    if interval[0] < kappa_score <= interval[1]: \n",
    "        print(f\"Interpretation of Kappa Score: {label}\") \n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81adb0f-a166-453c-9386-b801b0c74b7d",
   "metadata": {},
   "source": [
    "--- \n",
    "End code. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
